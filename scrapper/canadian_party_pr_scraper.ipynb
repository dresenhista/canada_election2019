{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Internet Scrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for scraping\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#to treat the data\n",
    "from dateutil.parser import parse\n",
    "import json\n",
    "\n",
    "#add data as dataframe and make math calculations\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading html with BeautifulSoup\n",
    "If you never scrapped the internet before BeautifulSoup is a great tool and it does the job. \n",
    "You will need to know how to translate html language to Soup though, this is how you can do it:\n",
    "\n",
    "| HTML  | SOUP|\n",
    "|---|---|\n",
    "|class = \"foo\"| .foo   |\n",
    "|id = \"bar\" |#bar|\n",
    "|<a href, >|  a |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions to collect all the urls for the conservative Party website (because we are using their website html flag)\n",
    "def get_news_url(news_website, section):\n",
    "    url_list = []\n",
    "    content = requests.get(news_website)\n",
    "    soup = BeautifulSoup(content.text, 'lxml')\n",
    "\n",
    "    #get the list of urls present in a website\n",
    "    for section in soup.select(section):\n",
    "        for a in section.findAll('a'):\n",
    "            #print(a.get_text())\n",
    "            #print(a.get('href'))\n",
    "            url_list.append(a.get('href'))\n",
    "    \n",
    "    return url_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now finally we will go through page by page to scrap the content\n",
    "#it returns two items, one is the content of the press release for the conservative party and \n",
    "#the second is the date that the press release happened.\n",
    "\n",
    "def get_content(webpage, class_name, class_date):\n",
    "    response = requests.get(webpage)\n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "    \n",
    "\n",
    "    return([soup.select(class_name)[0].get_text(), soup.select(class_date)[0].get_text()])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conservatives Scrapping first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conservative party of Canada URLs:\n",
    "#Get all the news category urls\n",
    "news_website = \"https://www.conservative.ca/news/\"\n",
    "news_website2 = \"https://www.conservative.ca/news/page/\"\n",
    "page_num = np.arange(2,61,1) #page starts on 2 and goes up to 10 1 at a time\n",
    "\n",
    "page_news_list = [news_website]\n",
    "for number in page_num:\n",
    "    a = news_website2+str(number)\n",
    "    page_news_list.append(a)\n",
    "\n",
    "#loop through the news page url getting all the news\n",
    "url_list = []\n",
    "for page in page_news_list:\n",
    "    url_list = url_list + get_news_url(page, '.section--news')\n",
    "\n",
    "#clean up the list   \n",
    "url_list = list(set(url_list)) #removes duplicates\n",
    "url_list = [item for item in url_list if item.count('page')==0] #removes the load more urls\n",
    "#looping through all the pages (this point might take a while since it opens each url)\n",
    "\n",
    "#scrap through the urls getting the content and the date\n",
    "content_scrapping = []\n",
    "content_date = []\n",
    "for url in url_list:\n",
    "    items= get_content(url, '.post-content', '.post-date')\n",
    "    content_scrapping.append(items[0])\n",
    "    content_date.append(items[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adds everything to a Dataframe and then transform the date string into an actual date type.\n",
    "conservatives_df = pd.DataFrame(data = {'party': 'conservative', 'url':url_list, \n",
    "                                        'content':content_scrapping, 'date': content_date})\n",
    "conservatives_df['date'] = conservatives_df.apply(lambda row: parse(row['date']), axis=1)\n",
    "\n",
    "#backup \n",
    "conservatives_df.to_csv('conservatives_df.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Liberal Scrapping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_website = \"https://www.liberal.ca/media-releases/\"\n",
    "page_num = np.arange(2,41,1) #page starts on 2 and goes up to 10 1 at a time\n",
    "\n",
    "page_news_list = [news_website]\n",
    "for number in page_num:\n",
    "    a = news_website+'page/'+str(number)\n",
    "    page_news_list.append(a)\n",
    "    \n",
    "liberal_url_list = []\n",
    "for page in page_news_list:\n",
    "    liberal_url_list = liberal_url_list + get_news_url(page, '.home-section')\n",
    "liberal_url_list = list(set(liberal_url_list)) #removes duplicates\n",
    "liberal_url_list = [item for item in liberal_url_list if item.count('page')==0] #removes the load more urls    \n",
    "liberal_url_list = [item for item in liberal_url_list if item.count('media-release')==0] #removes the load more urls  \n",
    "\n",
    "#looping through all the pages (this point might take a while since it opens each url)\n",
    "content_liberal = []\n",
    "content_date_liberal = []\n",
    "for url in liberal_url_list:\n",
    "    #print(url)\n",
    "    items= get_content(url, '.blog-content', '.byline')\n",
    "    content_liberal.append(items[0])\n",
    "    content_date_liberal.append(items[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adds everything to a Dataframe and then transform the date string into an actual date type.\n",
    "liberal_df = pd.DataFrame(data = {'party': 'liberal', 'url':liberal_url_list, \n",
    "                                        'content':content_liberal, 'date': content_date_liberal})\n",
    "liberal_df['date'] = liberal_df.apply(lambda row: parse(row['date']), axis=1)\n",
    "\n",
    "#liberal backup\n",
    "liberal_df.to_csv('liberal_df.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NDP Scrapping\n",
    "Of course they are different -_-. The NDP website doesnt have pagination, luckly the load more button makes a request that returns a json file with all the news links which is what we will be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates the new pagination\n",
    "news_generic = 'https://www.ndp.ca/latest?action_handler=canadandp-home/block--news-list&action=block--news-list--more&json=1&page='\n",
    "page_num = np.arange(2,15,1) #page starts on 2 and goes up to 10 1 at a time\n",
    "page_news_list = []\n",
    "for number in page_num:\n",
    "    a = news_generic+str(number)\n",
    "    page_news_list.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to get json data from NDP press release\n",
    "def pull_ndp_news_url(webpage):\n",
    "    ndp_url_list = []\n",
    "    response = requests.get(webpage)\n",
    "    data = json.loads(response.text)['list']\n",
    "    for item in data:\n",
    "        #print(item['link'])\n",
    "        ndp_url_list.append(item['link'])\n",
    "    return ndp_url_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gets the list of all press releases by the NDP\n",
    "ndp_url_list=[]\n",
    "for page in page_news_list:\n",
    "    ndp_url_list=ndp_url_list +pull_ndp_news_url(page)\n",
    "    \n",
    "#removing all the repeated urls from ndp\n",
    "ndp_url_list = list(set(ndp_url_list)) #removes duplicates\n",
    "\n",
    "#looping through all the pages to get their content\n",
    "ndp_content = []\n",
    "ndp_date = []\n",
    "for url in ndp_url_list:\n",
    "    \n",
    "    items= get_content(url, '.news2-body', '.news2-date')\n",
    "    ndp_content.append(items[0])\n",
    "    ndp_date.append(items[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adds everything to a Dataframe and then transform the date string into an actual date type.\n",
    "ndp_df = pd.DataFrame(data = {'party': 'NDP', 'url': ndp_url_list, \n",
    "                                        'content':ndp_content, 'date': ndp_date})\n",
    "ndp_df['date'] = ndp_df.apply(lambda row: parse(row['date']), axis=1)\n",
    "\n",
    "#backup\n",
    "ndp_df.to_csv('ndp_df.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>party</th>\n",
       "      <th>url</th>\n",
       "      <th>content</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>conservative</td>\n",
       "      <td>https://www.conservative.ca/the-hon-andrew-sch...</td>\n",
       "      <td>\\nFOR IMMEDIATE RELEASE\\nOttawa, ON – The Hon....</td>\n",
       "      <td>2019-07-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>conservative</td>\n",
       "      <td>https://www.conservative.ca/justin-trudeau-mus...</td>\n",
       "      <td>\\nFOR IMMEDIATE RELEASE\\n\\nTORONTO, ON\\n– Toda...</td>\n",
       "      <td>2019-07-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>conservative</td>\n",
       "      <td>https://www.conservative.ca/justin-trudeaus-ba...</td>\n",
       "      <td>\\nFOR IMMEDIATE RELEASE\\nOTTAWA, ON – Pierre P...</td>\n",
       "      <td>2019-05-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>conservative</td>\n",
       "      <td>https://www.conservative.ca/conservatives-to-s...</td>\n",
       "      <td>\\nNew conservative government to extend EI par...</td>\n",
       "      <td>2019-10-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>conservative</td>\n",
       "      <td>https://www.conservative.ca/andrew-scheer-reve...</td>\n",
       "      <td>\\nConservatives unveil fully costed platform t...</td>\n",
       "      <td>2019-10-11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          party                                                url  \\\n",
       "0  conservative  https://www.conservative.ca/the-hon-andrew-sch...   \n",
       "1  conservative  https://www.conservative.ca/justin-trudeau-mus...   \n",
       "2  conservative  https://www.conservative.ca/justin-trudeaus-ba...   \n",
       "3  conservative  https://www.conservative.ca/conservatives-to-s...   \n",
       "4  conservative  https://www.conservative.ca/andrew-scheer-reve...   \n",
       "\n",
       "                                             content       date  \n",
       "0  \\nFOR IMMEDIATE RELEASE\\nOttawa, ON – The Hon.... 2019-07-11  \n",
       "1  \\nFOR IMMEDIATE RELEASE\\n\\nTORONTO, ON\\n– Toda... 2019-07-11  \n",
       "2  \\nFOR IMMEDIATE RELEASE\\nOTTAWA, ON – Pierre P... 2019-05-28  \n",
       "3  \\nNew conservative government to extend EI par... 2019-10-15  \n",
       "4  \\nConservatives unveil fully costed platform t... 2019-10-11  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Concatenating everything\n",
    "frames = [conservatives_df, liberal_df, ndp_df]\n",
    "df = pd.concat(frames)\n",
    "#backup\n",
    "df.to_csv('canada_parties_pr.csv', index = False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pulls news from a generic news paper to be the hold out data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Toronto star\n",
    "#creates the new pagination\n",
    "webpage = 'https://www.thestar.com/news/canada/2020/04/18/canada-among-13-countries-uniting-to-demand-global-co-operation-russia-reports-biggest-increase-in-cases-spain-becomes-third-country-to-report-over-20000-virus-deaths.html'\n",
    "response = requests.get(webpage)\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "class_name='.text-block-container'\n",
    "\n",
    "article = ''\n",
    "for text in soup.select(class_name):\n",
    "    article  += text.get_text()\n",
    "article_df =  pd.DataFrame(data= {'article':[article]})\n",
    "article_df.to_csv('torontostar_sample.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
