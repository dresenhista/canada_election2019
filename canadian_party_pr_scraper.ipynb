{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for scraping\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#to treat the data\n",
    "from dateutil.parser import parse\n",
    "import json\n",
    "\n",
    "#add data as dataframe and make math calculations\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions to collect all the urls for the conservative Party website (because we are using their website html flag)\n",
    "def get_news_url(news_website, section):\n",
    "    url_list = []\n",
    "    content = requests.get(news_website)\n",
    "    soup = BeautifulSoup(content.text, 'lxml')\n",
    "\n",
    "    #get the list of urls present in a website\n",
    "    for section in soup.select(section):\n",
    "        for a in section.findAll('a'):\n",
    "            #print(a.get_text())\n",
    "            #print(a.get('href'))\n",
    "            url_list.append(a.get('href'))\n",
    "    driver.quit()\n",
    "    return url_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now finally we will go through page by page to scrap the content\n",
    "#it returns two items, one is the content of the press release for the conservative party and \n",
    "#the second is the date that the press release happened.\n",
    "\n",
    "def get_content(webpage, class_name, class_date):\n",
    "    response = requests.get(webpage)\n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "    driver.quit()\n",
    "\n",
    "    return([soup.select(class_name)[0].get_text(), soup.select(class_date)[0].get_text()])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conservatives Scrapping first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conservative party of Canada URLs:\n",
    "#Get all the news category urls\n",
    "news_website = \"https://www.conservative.ca/news/\"\n",
    "news_website2 = \"https://www.conservative.ca/news/page/\"\n",
    "page_num = np.arange(2,21,1) #page starts on 2 and goes up to 10 1 at a time\n",
    "\n",
    "page_news_list = [news_website]\n",
    "for number in page_num:\n",
    "    a = news_website2+str(number)\n",
    "    page_news_list.append(a)\n",
    "\n",
    "#loop through the news page url getting all the news\n",
    "url_list = []\n",
    "for page in page_news_list:\n",
    "    url_list = url_list + get_news_url(page, '.section--news')\n",
    "\n",
    "#clean up the list   \n",
    "url_list = list(set(url_list)) #removes duplicates\n",
    "url_list = [item for item in url_list if item.count('page')==0] #removes the load more urls\n",
    "#looping through all the pages (this point might take a while since it opens each url)\n",
    "\n",
    "#scrap through the urls getting the content and the date\n",
    "content_scrapping = []\n",
    "content_date = []\n",
    "for url in url_list:\n",
    "    items= get_content(url, '.post-content', '.post-date')\n",
    "    content_scrapping.append(items[0])\n",
    "    content_date.append(items[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adds everything to a Dataframe and then transform the date string into an actual date type.\n",
    "conservatives_df = pd.DataFrame(data = {'party': 'conservative', 'url':url_list, \n",
    "                                        'content':content_scrapping, 'date': content_date})\n",
    "conservatives_df['date'] = conservatives_df.apply(lambda row: parse(row['date']), axis=1)\n",
    "\n",
    "#backup \n",
    "conservatives_df.to_csv('conservatives_df.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Liberal Scrapping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_website = \"https://www.liberal.ca/media-releases/\"\n",
    "page_num = np.arange(2,21,1) #page starts on 2 and goes up to 10 1 at a time\n",
    "\n",
    "page_news_list = [news_website]\n",
    "for number in page_num:\n",
    "    a = news_website+'page/'+str(number)\n",
    "    page_news_list.append(a)\n",
    "    \n",
    "liberal_url_list = []\n",
    "for page in page_news_list:\n",
    "    liberal_url_list = liberal_url_list + get_news_url(page, '.home-section')\n",
    "liberal_url_list = list(set(liberal_url_list)) #removes duplicates\n",
    "liberal_url_list = [item for item in liberal_url_list if item.count('page')==0] #removes the load more urls    \n",
    "liberal_url_list = [item for item in liberal_url_list if item.count('media-releases')==0] #removes the load more urls  \n",
    "\n",
    "#looping through all the pages (this point might take a while since it opens each url)\n",
    "content_liberal = []\n",
    "content_date_liberal = []\n",
    "for url in liberal_url_list:\n",
    "    #print(url)\n",
    "    items= get_content(url, '.blog-content', '.byline')\n",
    "    content_liberal.append(items[0])\n",
    "    content_date_liberal.append(items[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adds everything to a Dataframe and then transform the date string into an actual date type.\n",
    "liberal_df = pd.DataFrame(data = {'party': 'liberal', 'url':liberal_url_list, \n",
    "                                        'content':content_liberal, 'date': content_date_liberal})\n",
    "liberal_df['date'] = liberal_df.apply(lambda row: parse(row['date']), axis=1)\n",
    "\n",
    "#liberal backup\n",
    "liberal_df.to_csv('liberal_df.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NDP Scrapping\n",
    "Of course they are different -_-. The NDP website doesnt have pagination, luckly the load more button makes a request that returns a json file with all the news links which is what we will be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates the new pagination\n",
    "news_generic = 'https://www.ndp.ca/latest?action_handler=canadandp-home/block--news-list&action=block--news-list--more&json=1&page='\n",
    "page_num = np.arange(2,15,1) #page starts on 2 and goes up to 10 1 at a time\n",
    "page_news_list = []\n",
    "for number in page_num:\n",
    "    a = news_generic+str(number)\n",
    "    page_news_list.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to get json data from NDP press release\n",
    "def pull_ndp_news_url(webpage):\n",
    "    ndp_url_list = []\n",
    "    response = requests.get(webpage)\n",
    "    data = json.loads(response.text)['list']\n",
    "    for item in data:\n",
    "        #print(item['link'])\n",
    "        ndp_url_list.append(item['link'])\n",
    "    return ndp_url_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gets the list of all press releases by the NDP\n",
    "ndp_url_list=[]\n",
    "for page in page_news_list:\n",
    "    ndp_url_list=ndp_url_list +pull_ndp_news_url(page)\n",
    "    \n",
    "#removing all the repeated urls from ndp\n",
    "ndp_url_list = list(set(ndp_url_list)) #removes duplicates\n",
    "\n",
    "#looping through all the pages to get their content\n",
    "ndp_content = []\n",
    "ndp_date = []\n",
    "for url in ndp_url_list:\n",
    "    \n",
    "    items= get_content(url, '.news2-body', '.news2-date')\n",
    "    ndp_content.append(items[0])\n",
    "    ndp_date.append(items[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adds everything to a Dataframe and then transform the date string into an actual date type.\n",
    "ndp_df = pd.DataFrame(data = {'party': 'NDP', 'url': ndp_url_list, \n",
    "                                        'content':ndp_content, 'date': ndp_date})\n",
    "ndp_df['date'] = ndp_df.apply(lambda row: parse(row['date']), axis=1)\n",
    "\n",
    "#backup\n",
    "ndp_df.to_csv('ndp_df.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>party</th>\n",
       "      <th>url</th>\n",
       "      <th>content</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>conservative</td>\n",
       "      <td>https://www.conservative.ca/a-new-conservative...</td>\n",
       "      <td>\\r\\nScheer to restore order, fairness, and com...</td>\n",
       "      <td>2019-10-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>conservative</td>\n",
       "      <td>https://www.conservative.ca/the-last-time-just...</td>\n",
       "      <td>\\r\\nTrudeau can’t be trusted to make life more...</td>\n",
       "      <td>2019-09-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>conservative</td>\n",
       "      <td>https://www.conservative.ca/the-hon-andrew-sch...</td>\n",
       "      <td>\\r\\nSEPTEMBER 29, 2019\\r\\nFOR IMMEDIATE RELEAS...</td>\n",
       "      <td>2019-09-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>conservative</td>\n",
       "      <td>https://www.conservative.ca/100-day-action-pla...</td>\n",
       "      <td>\\r\\nFamilies will start saving hundreds of dol...</td>\n",
       "      <td>2019-10-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>conservative</td>\n",
       "      <td>https://www.conservative.ca/justin-trudeau-ref...</td>\n",
       "      <td>\\r\\nTrudeau’s plan means less coverage and hig...</td>\n",
       "      <td>2019-09-23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          party                                                url  \\\n",
       "0  conservative  https://www.conservative.ca/a-new-conservative...   \n",
       "1  conservative  https://www.conservative.ca/the-last-time-just...   \n",
       "2  conservative  https://www.conservative.ca/the-hon-andrew-sch...   \n",
       "3  conservative  https://www.conservative.ca/100-day-action-pla...   \n",
       "4  conservative  https://www.conservative.ca/justin-trudeau-ref...   \n",
       "\n",
       "                                             content        date  \n",
       "0  \\r\\nScheer to restore order, fairness, and com...  2019-10-09  \n",
       "1  \\r\\nTrudeau can’t be trusted to make life more...  2019-09-30  \n",
       "2  \\r\\nSEPTEMBER 29, 2019\\r\\nFOR IMMEDIATE RELEAS...  2019-09-29  \n",
       "3  \\r\\nFamilies will start saving hundreds of dol...  2019-10-17  \n",
       "4  \\r\\nTrudeau’s plan means less coverage and hig...  2019-09-23  "
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Concatenating everything\n",
    "frames = [conservative_df, liberal_df, ndp_df]\n",
    "df = pd.concat(frames)\n",
    "#backup\n",
    "df.to_csv('canada_parties_pr.csv', index = False)\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
